{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "\n",
    "with open(\"../../data/article_texts.txt\",'rb') as f:\n",
    "    texts = pickle.load(f, encoding=\"UTF-8\")\n",
    "with open(\"../../data/english_anecs_list.pickle\", \"rb\") as f:\n",
    "    english_anecs_list = pickle.load(f, encoding=\"UTF-8\")\n",
    "with open(\"../../data/ids_to_labels.pickle\", \"rb\") as f:\n",
    "    ids_to_labels = pickle.load(f, encoding=\"utf-8\")\n",
    "with open(\"../../data/labels_to_ids.pickle\", \"rb\") as f:\n",
    "    labels_to_ids = pickle.load(f, encoding=\"utf-8\")\n",
    "with open(\"../../data/unique_labels.pickle\", \"rb\") as f:\n",
    "    unique_labels = pickle.load(f, encoding=\"UTF-8\")\n",
    "with open(\"../../data/translated_anecs.txt\", \"r\") as f:\n",
    "    translated_anecs = f.read().replace(\"<unk> \", \"\").replace(\"♪ \", \"\").split(\"\\n\")\n",
    "with open(\"../../data/translated_anecs_prepared.pickle\", \"rb\") as f:\n",
    "    translated_anecs_prepared = pickle.load(f)\n",
    "with open(\"../../data/english_anecs_prepared.pickle\", \"rb\") as f:\n",
    "    english_anecs_prepared = pickle.load(f)\n",
    "with open(\"../../data/glove_vectors.pickle\", \"rb\") as f:\n",
    "    glove_vectors = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me, is that your Jaguar parked outside? Yes I 'll finish. \n"
     ]
    }
   ],
   "source": [
    "print(translated_anecs[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "text = ['SpaceX Starship Blows Up Minutes After Launch',\n",
    " 'SpaceX’s Starship rocket, the most powerful ever built, blasted off on an unpiloted maiden flight Thursday, flying for more than two minutes before exploding. What do you think?']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dslim/bert-base-NER')\n",
    "\n",
    "def tokenize(data:str):\n",
    "    inputs = tokenizer(data, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    return inputs\n",
    "def ids_to_tokens(text_input):\n",
    "    return tokenizer.convert_ids_to_tokens(text_input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dslim/bert-base-NER and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([17, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([17]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "import torch\n",
    "\n",
    "class BertModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertModel, self).__init__()\n",
    "        self.bert = BertForTokenClassification.from_pretrained('dslim/bert-base-NER', num_labels=len(unique_labels),\n",
    "                                                               ignore_mismatched_sizes=True)\n",
    "\n",
    "    def forward(self, input_ids, label=None):\n",
    "        output = self.bert(labels=label, input_ids=input_ids, return_dict=False)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "model = BertModel()\n",
    "\n",
    "model.load_state_dict(torch.load('../models/bert_trainedNEREnglish', map_location=torch.device('cpu')))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Space B-org\n",
      "##X I-org\n",
      "’ O\n",
      "s O\n",
      "Stars O\n",
      "##hip O\n",
      "rocket O\n",
      ", O\n",
      "the O\n",
      "most O\n",
      "powerful O\n",
      "ever O\n",
      "built O\n",
      ", O\n",
      "blasted O\n",
      "off O\n",
      "on O\n",
      "an O\n",
      "un O\n",
      "##pi O\n",
      "##lot O\n",
      "##ed O\n",
      "maiden O\n",
      "flight O\n",
      "Thursday B-tim\n",
      ", O\n",
      "flying O\n",
      "for O\n",
      "more O\n",
      "than O\n",
      "two O\n",
      "minutes O\n",
      "before O\n",
      "exploding O\n",
      ". O\n",
      "What O\n",
      "do O\n",
      "you O\n",
      "think O\n",
      "? O\n",
      "[{'word': 'SpaceX', 'entity': 'B-org'}, {'word': '’', 'entity': 'O'}, {'word': 's', 'entity': 'O'}, {'word': 'Starship', 'entity': 'O'}, {'word': 'rocket', 'entity': 'O'}, {'word': ',', 'entity': 'O'}, {'word': 'the', 'entity': 'O'}, {'word': 'most', 'entity': 'O'}, {'word': 'powerful', 'entity': 'O'}, {'word': 'ever', 'entity': 'O'}, {'word': 'built', 'entity': 'O'}, {'word': ',', 'entity': 'O'}, {'word': 'blasted', 'entity': 'O'}, {'word': 'off', 'entity': 'O'}, {'word': 'on', 'entity': 'O'}, {'word': 'an', 'entity': 'O'}, {'word': 'unpiloted', 'entity': 'O'}, {'word': 'maiden', 'entity': 'O'}, {'word': 'flight', 'entity': 'O'}, {'word': 'Thursday', 'entity': 'B-tim'}, {'word': ',', 'entity': 'O'}, {'word': 'flying', 'entity': 'O'}, {'word': 'for', 'entity': 'O'}, {'word': 'more', 'entity': 'O'}, {'word': 'than', 'entity': 'O'}, {'word': 'two', 'entity': 'O'}, {'word': 'minutes', 'entity': 'O'}, {'word': 'before', 'entity': 'O'}, {'word': 'exploding', 'entity': 'O'}, {'word': '.', 'entity': 'O'}, {'word': 'What', 'entity': 'O'}, {'word': 'do', 'entity': 'O'}, {'word': 'you', 'entity': 'O'}, {'word': 'think', 'entity': 'O'}, {'word': '?', 'entity': 'O'}]\n",
      "SpaceX ’ s Starship rocket , the most powerful ever built , blasted off on an unpiloted maiden flight Thursday , flying for more than two minutes before exploding . What do you think ? \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def get_ners(text:str) -> list:\n",
    "    output = []\n",
    "    B = np.asarray([tokenizer(text.replace(\"-\", \"\"))[\"input_ids\"]]).reshape(1,1,-1)\n",
    "    logits = model(torch.as_tensor(np.array(B))[0])[0]\n",
    "    for j in range(logits.shape[0]):\n",
    "        #print(logits[i])\n",
    "        logits_clean = logits[j].argmax(dim=1)\n",
    "        words = text.replace(\"-\", \"\").split()\n",
    "        tokenized_sentence = ids_to_tokens(tokenizer(text.replace(\"-\", \"\"))[\"input_ids\"])\n",
    "        for i in range(len(logits_clean[1:-1])):\n",
    "            print(tokenized_sentence[i+1], ids_to_labels[logits_clean[i+1].item()])\n",
    "        #print([ids_to_labels[x.item()] for x in logits_clean])\n",
    "        i = 1\n",
    "        for elem in logits_clean[1:-1]:\n",
    "            if i > 1 and (tokenized_sentence[i][:2] == \"##\" or ids_to_labels[elem.item()][0] == \"I\") :\n",
    "                if tokenized_sentence[i][:2] == \"##\":\n",
    "                    output[-1][\"word\"] += tokenized_sentence[i][2:]\n",
    "                else:\n",
    "                    output[-1][\"word\"] += tokenized_sentence[i]\n",
    "            else:\n",
    "                output.append({\"word\":tokenized_sentence[i], \"entity\":ids_to_labels[elem.item()]})\n",
    "            i += 1\n",
    "        \"\"\"\n",
    "        for el in logits_clean:\n",
    "            if i == len(words):\n",
    "                break\n",
    "            elem = logits_clean[k]\n",
    "            if k+1 <= len(logits_clean) and tokenized_sentence[k][:2] == \"##\":\n",
    "                if elem.item() == \"O\":\n",
    "                    label = logits_clean[k]\n",
    "                else:\n",
    "                    label = elem.item()\n",
    "                output.append({\"word\":words[i], \"entity\":ids_to_labels[label]})\n",
    "                k+=2\n",
    "            else:\n",
    "                output.append({\"word\":words[i], \"entity\":ids_to_labels[elem.item()]})\n",
    "                k+=1\n",
    "            i += 1\n",
    "        \"\"\"\n",
    "    return output\n",
    "out = get_ners(texts[0][1])\n",
    "print(out)\n",
    "temp = \"\"\n",
    "for elem in out:\n",
    "    temp += elem[\"word\"] + \" \"\n",
    "print(temp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def get_embeddings(list_of_tags: list):\n",
    "    emeddings = []\n",
    "    for tag in list_of_tags:\n",
    "        try:\n",
    "            #print(tokenizer.convert_ids_to_tokens(tokenizer(tag[\"word\"])[\"input_ids\"][1]))\n",
    "            embed = glove_vectors[tag[\"word\"]]\n",
    "            emeddings.append({'entity': tag[\"entity\"] , 'word': tag[\"word\"], \"embedding\": embed})\n",
    "        except:\n",
    "            emeddings.append({'entity': tag[\"entity\"] , 'word': tag[\"word\"], \"embedding\": glove_vectors[\"base\"]})\n",
    "            #print(\"Broken embedding\", tag[\"word\"], tag[\"entity\"])\n",
    "    return emeddings\n",
    "def get_non_o(ner_words):\n",
    "    a = []\n",
    "    for x in ner_words:\n",
    "        if x[\"entity\"] != \"O\" :\n",
    "            a.append(x)\n",
    "    return a\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "from collections import Counter\n",
    "\n",
    "def count_cos_embeddings(text_embeddings, anec_embeddings) -> (float, dict):\n",
    "    suitable_pairs = []\n",
    "    anec_unique_tags_counter = Counter()\n",
    "    embedding_cosine_sum = 0\n",
    "    for embedding in text_embeddings:\n",
    "        cosines = []\n",
    "        pair = \"\"\n",
    "        simmilarity_tags = {}\n",
    "        for embed in anec_embeddings:\n",
    "            anec_unique_tags_counter[embed[\"entity\"]] += 1\n",
    "            #print(f\"{embed['word']}:{embed['entity']}\", f\"{embedding['word']}:{embedding['entity']}\")\n",
    "            if embed[\"entity\"] == embedding[\"entity\"]:\n",
    "                v1 = embedding[\"embedding\"]\n",
    "                v2 = embed[\"embedding\"]\n",
    "                cos = np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "                cosines.append((cos, embed[\"word\"], embed[\"entity\"] ))\n",
    "        simmilarity_tags[embedding[\"word\"]] = cosines\n",
    "        suitable_pairs.append(simmilarity_tags)\n",
    "    top_similarity = {}\n",
    "    for word_dict in suitable_pairs:\n",
    "        word = list(word_dict.keys())[0]\n",
    "        top = (-10, \"\")\n",
    "        key = None\n",
    "        for sim in word_dict[word]:\n",
    "            if top[0] < sim[0]:\n",
    "                top = (sim[0], word)\n",
    "                key = sim[1]\n",
    "        if key:\n",
    "            top_similarity[key] = top\n",
    "    #print(top_similarity)\n",
    "    #print(suitable_pairs)\n",
    "    for key in top_similarity.keys():\n",
    "        embedding_cosine_sum += top_similarity[key][0]\n",
    "    embedding_cosine_sum /= len(top_similarity.keys()) if len(top_similarity.keys())>5 else 5\n",
    "    return embedding_cosine_sum, top_similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr B-per\n",
      ". I-per\n",
      "Cut I-per\n",
      "##ter I-per\n",
      "is O\n",
      "the O\n",
      "local O\n",
      "V O\n",
      "##eter O\n",
      "##ina O\n",
      "##rian O\n",
      ", O\n",
      "known O\n",
      "for O\n",
      "his O\n",
      "w O\n",
      "##ry O\n",
      "humor O\n",
      ". O\n",
      "He O\n",
      "surpassed O\n",
      "himself O\n",
      "one O\n",
      "summer O\n",
      "day O\n",
      "when O\n",
      "a O\n",
      "city O\n",
      "dog O\n",
      "was O\n",
      "brought O\n",
      "to O\n",
      "him O\n",
      "after O\n",
      "an O\n",
      "encounter O\n",
      "with O\n",
      "a O\n",
      "p O\n",
      "##or O\n",
      "##cup O\n",
      "##ine O\n",
      ". O\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m filtred_anec \u001B[38;5;241m=\u001B[39m get_non_o(get_ners(english_anecs_list[\u001B[38;5;241m30\u001B[39m]))\n\u001B[1;32m----> 2\u001B[0m filtred_text \u001B[38;5;241m=\u001B[39m get_non_o(\u001B[43mget_ners\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m      3\u001B[0m anec_embeddings \u001B[38;5;241m=\u001B[39m get_embeddings(filtred_anec)\n\u001B[0;32m      4\u001B[0m text_embeddings \u001B[38;5;241m=\u001B[39m get_embeddings(filtred_text)\n",
      "Cell \u001B[1;32mIn[6], line 4\u001B[0m, in \u001B[0;36mget_ners\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_ners\u001B[39m(text:\u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[0;32m      3\u001B[0m     output \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m----> 4\u001B[0m     B \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray([tokenizer(\u001B[43mtext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplace\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m))[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]])\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      5\u001B[0m     logits \u001B[38;5;241m=\u001B[39m model(torch\u001B[38;5;241m.\u001B[39mas_tensor(np\u001B[38;5;241m.\u001B[39marray(B))[\u001B[38;5;241m0\u001B[39m])[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(logits\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]):\n\u001B[0;32m      7\u001B[0m         \u001B[38;5;66;03m#print(logits[i])\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'list' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "filtred_anec = get_non_o(get_ners(english_anecs_list[30]))\n",
    "filtred_text = get_non_o(get_ners(text))\n",
    "anec_embeddings = get_embeddings(filtred_anec)\n",
    "text_embeddings = get_embeddings(filtred_text)\n",
    "new_cosine, new_simmilarity = count_cos_embeddings(text_embeddings, anec_embeddings)\n",
    "#print(filtred_anec, filtred_text)\n",
    "print(new_simmilarity)\n",
    "#print(text_embeddings, anec_embeddings)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Law O\n",
      "##yers O\n",
      "for O\n",
      "F B-org\n",
      "##T B-org\n",
      "##X I-org\n",
      "founder O\n",
      "Sam B-per\n",
      "Bank I-per\n",
      "##man I-per\n",
      "##F I-per\n",
      "##ried I-per\n",
      "on O\n",
      "Monday O\n",
      "filed O\n",
      "motions O\n",
      "to O\n",
      "dismiss O\n",
      "the O\n",
      "US B-geo\n",
      "government O\n",
      "’ O\n",
      "s O\n",
      "fraud O\n",
      "charges O\n",
      "against O\n",
      "him O\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'word': 'FTX', 'entity': 'B-org'},\n {'word': 'SamBankmanFried', 'entity': 'B-per'},\n {'word': 'US', 'entity': 'B-geo'}]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anec = english_anecs_list[5]\n",
    "text = texts[12][1]\n",
    "#get_ners(anec)\n",
    "get_non_o(get_ners(\"Lawyers for FTX founder Sam Bankman-Fried on Monday filed motions to dismiss the US government’s fraud charges against him\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_anecs_prepared"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The O\n",
      "Tibetan B-gpe\n",
      "spiritual O\n",
      "leader O\n",
      "the O\n",
      "Dal B-per\n",
      "##ai B-per\n",
      "Lama I-per\n",
      "apologized O\n",
      "Monday O\n",
      "after O\n",
      "a O\n",
      "video O\n",
      "that O\n",
      "showed O\n",
      "him O\n",
      "asking O\n",
      "a O\n",
      "boy O\n",
      "to O\n",
      "suck O\n",
      "his O\n",
      "tongue O\n",
      "triggered O\n",
      "a O\n",
      "back O\n",
      "##lash O\n",
      "on O\n",
      "social O\n",
      "media O\n",
      ". O\n",
      "What O\n",
      "do O\n",
      "you O\n",
      "think O\n",
      "? O\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4158 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "069d67b882d1477b986edd6a61635ac1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of the most tactful people on Earth are English.  One  office supervisor called a secretary in to give her the bad news  that she was being fired.  He started the conversation with:  \"Miss Symthe, I really don't know how we're going to get along  without you, but starting Monday, we're going to try. 0.4 [{'word': 'Tibetan', 'entity': 'B-gpe'}, {'word': 'DalaiLama', 'entity': 'B-per'}] [{'entity': 'B-gpe', 'word': 'English.', 'embedding': array([-0.85078  ,  0.14052  ,  0.19218  , -0.56941  ,  0.012045 ,\n",
      "       -0.13574  ,  0.026476 ,  0.63774  ,  0.13145  , -1.5406   ,\n",
      "        0.022069 , -0.093937 , -0.0075906,  0.18398  ,  0.098117 ,\n",
      "        0.34659  , -0.29379  , -0.023845 ,  0.34028  , -0.16479  ,\n",
      "       -0.4635   , -0.7426   , -0.098289 , -0.66905  , -0.20917  ,\n",
      "       -0.097382 , -0.028347 ,  0.5289   , -0.1507   ,  0.52132  ,\n",
      "        0.096284 , -0.20201  , -0.23403  ,  0.35867  ,  0.14002  ,\n",
      "        0.13554  ,  0.15319  ,  0.32805  ,  0.020936 ,  0.1657   ,\n",
      "       -0.0089688,  0.059181 , -0.1375   ,  0.035753 , -0.17763  ,\n",
      "        0.12395  ,  0.041585 ,  0.51095  ,  0.11174  ,  0.030507 ,\n",
      "       -0.079747 , -0.70903  , -0.60473  ,  0.36105  , -0.16732  ,\n",
      "       -0.044104 ,  0.10538  ,  0.39964  ,  0.22989  ,  0.31049  ,\n",
      "       -0.24369  , -0.33433  ,  0.3908   , -0.52597  , -0.44613  ,\n",
      "       -0.055256 , -0.084394 , -0.28462  ,  0.36583  ,  0.62191  ,\n",
      "       -0.36241  ,  0.054556 ,  0.42029  ,  0.2916   , -0.68549  ,\n",
      "        0.29185  ,  0.60148  , -0.30741  ,  0.4329   , -0.27715  ,\n",
      "        0.40411  ,  0.27686  , -0.13243  , -0.31177  ,  0.061525 ,\n",
      "       -0.15922  ,  0.19996  , -0.39375  ,  0.25412  , -0.10159  ,\n",
      "        0.6096   ,  0.12271  , -0.59604  , -0.31485  , -0.32325  ,\n",
      "       -0.14606  , -0.25834  ,  0.20505  ,  0.1371   ,  0.19961  ,\n",
      "       -0.1093   , -0.24252  ,  0.10222  ,  0.21871  ,  0.037622 ,\n",
      "       -0.80619  ,  0.56828  ,  0.18972  , -0.18194  ,  0.57378  ,\n",
      "        0.27703  , -0.99991  , -0.15494  , -0.80141  , -0.031519 ,\n",
      "        0.077413 , -0.38559  ,  0.33344  , -0.34985  , -0.0061697,\n",
      "        0.26873  , -0.012963 ,  0.43523  , -0.35281  , -0.18652  ,\n",
      "       -0.12175  ,  0.024119 ,  0.075614 ,  0.27854  , -0.30625  ,\n",
      "       -0.3751   ,  0.48567  , -0.90504  , -0.086511 ,  0.2357   ,\n",
      "       -0.51093  , -0.28514  , -0.25993  ,  0.70909  , -0.41138  ,\n",
      "       -0.41395  , -0.0059017, -0.31946  ,  0.0089687, -0.34839  ,\n",
      "        0.22888  , -0.078148 ,  0.11172  , -0.18629  ,  0.11579  ,\n",
      "        1.0416   , -0.74788  ,  0.33463  , -0.56744  ,  0.5173   ,\n",
      "       -0.34971  ,  0.2192   , -0.067904 , -0.1231   ,  0.45712  ,\n",
      "       -0.03048  , -0.39132  , -0.11778  ,  0.14276  ,  0.91607  ,\n",
      "       -0.45087  , -0.23127  ,  0.30308  ,  0.26162  ,  0.11788  ,\n",
      "       -0.2151   , -0.25472  ,  0.51046  , -0.18225  , -0.23448  ,\n",
      "        0.63514  ,  0.48773  ,  0.13602  ,  0.077952 ,  0.048624 ,\n",
      "        0.37765  ,  0.30025  ,  0.32011  , -0.29244  , -0.060639 ,\n",
      "        0.38323  ,  0.14346  ,  0.041774 , -0.1738   ,  0.29323  ,\n",
      "       -0.18339  ,  0.89149  ,  0.7064   , -0.48768  ,  0.60736  ,\n",
      "       -0.050338 ,  0.072991 , -0.15112  , -0.45522  ,  0.33195  ,\n",
      "        1.3761   , -0.74979  ,  0.2557   ,  0.29944  , -0.045796 ,\n",
      "        0.30665  ,  0.45234  , -0.017505 , -0.51614  ,  0.47881  ,\n",
      "       -0.099928 , -0.49045  , -0.0175   , -0.0035313, -0.45266  ,\n",
      "        0.17951  , -0.53411  ,  0.36321  , -0.14557  ,  0.50801  ,\n",
      "        0.30895  ,  0.45874  ,  0.37383  , -0.22473  , -0.23064  ,\n",
      "        0.30568  , -0.15955  ,  0.23174  ,  0.22386  , -0.90902  ,\n",
      "       -0.047653 , -0.58221  ,  0.44276  ,  0.48688  , -0.020569 ,\n",
      "        0.015207 , -0.36565  , -0.056182 , -0.59864  ,  0.39156  ,\n",
      "       -0.1214   ,  0.081708 ,  0.8414   , -0.33834  , -0.57516  ,\n",
      "       -0.43614  , -0.027974 ,  0.38013  , -0.1152   , -0.15773  ,\n",
      "       -0.45423  ,  0.086106 ,  0.28252  , -0.14643  ,  0.61413  ,\n",
      "       -0.084234 , -0.13932  , -0.25247  , -0.51641  , -0.11502  ,\n",
      "        0.37162  , -0.061126 , -0.22138  , -0.11358  , -0.61499  ,\n",
      "       -0.073034 ,  0.20285  ,  0.65178  ,  0.45677  , -0.10379  ,\n",
      "        0.018831 ,  0.38626  ,  0.014366 , -0.18982  , -0.4907   ,\n",
      "       -0.044281 , -1.356    , -0.015133 ,  0.15155  , -0.073018 ,\n",
      "       -0.46388  ,  0.14422  , -0.31741  , -0.23018  , -0.414    ,\n",
      "        0.23789  , -0.085779 ,  0.21384  ,  0.3762   ,  0.34505  ,\n",
      "       -0.12657  , -0.59409  , -0.46864  , -0.32798  , -0.018532 ,\n",
      "        0.77841  , -0.19691  , -0.80945  , -0.38679  ,  0.11802  ],\n",
      "      dtype=float32)}, {'entity': 'B-per', 'word': \"don't\", 'embedding': array([-0.85078  ,  0.14052  ,  0.19218  , -0.56941  ,  0.012045 ,\n",
      "       -0.13574  ,  0.026476 ,  0.63774  ,  0.13145  , -1.5406   ,\n",
      "        0.022069 , -0.093937 , -0.0075906,  0.18398  ,  0.098117 ,\n",
      "        0.34659  , -0.29379  , -0.023845 ,  0.34028  , -0.16479  ,\n",
      "       -0.4635   , -0.7426   , -0.098289 , -0.66905  , -0.20917  ,\n",
      "       -0.097382 , -0.028347 ,  0.5289   , -0.1507   ,  0.52132  ,\n",
      "        0.096284 , -0.20201  , -0.23403  ,  0.35867  ,  0.14002  ,\n",
      "        0.13554  ,  0.15319  ,  0.32805  ,  0.020936 ,  0.1657   ,\n",
      "       -0.0089688,  0.059181 , -0.1375   ,  0.035753 , -0.17763  ,\n",
      "        0.12395  ,  0.041585 ,  0.51095  ,  0.11174  ,  0.030507 ,\n",
      "       -0.079747 , -0.70903  , -0.60473  ,  0.36105  , -0.16732  ,\n",
      "       -0.044104 ,  0.10538  ,  0.39964  ,  0.22989  ,  0.31049  ,\n",
      "       -0.24369  , -0.33433  ,  0.3908   , -0.52597  , -0.44613  ,\n",
      "       -0.055256 , -0.084394 , -0.28462  ,  0.36583  ,  0.62191  ,\n",
      "       -0.36241  ,  0.054556 ,  0.42029  ,  0.2916   , -0.68549  ,\n",
      "        0.29185  ,  0.60148  , -0.30741  ,  0.4329   , -0.27715  ,\n",
      "        0.40411  ,  0.27686  , -0.13243  , -0.31177  ,  0.061525 ,\n",
      "       -0.15922  ,  0.19996  , -0.39375  ,  0.25412  , -0.10159  ,\n",
      "        0.6096   ,  0.12271  , -0.59604  , -0.31485  , -0.32325  ,\n",
      "       -0.14606  , -0.25834  ,  0.20505  ,  0.1371   ,  0.19961  ,\n",
      "       -0.1093   , -0.24252  ,  0.10222  ,  0.21871  ,  0.037622 ,\n",
      "       -0.80619  ,  0.56828  ,  0.18972  , -0.18194  ,  0.57378  ,\n",
      "        0.27703  , -0.99991  , -0.15494  , -0.80141  , -0.031519 ,\n",
      "        0.077413 , -0.38559  ,  0.33344  , -0.34985  , -0.0061697,\n",
      "        0.26873  , -0.012963 ,  0.43523  , -0.35281  , -0.18652  ,\n",
      "       -0.12175  ,  0.024119 ,  0.075614 ,  0.27854  , -0.30625  ,\n",
      "       -0.3751   ,  0.48567  , -0.90504  , -0.086511 ,  0.2357   ,\n",
      "       -0.51093  , -0.28514  , -0.25993  ,  0.70909  , -0.41138  ,\n",
      "       -0.41395  , -0.0059017, -0.31946  ,  0.0089687, -0.34839  ,\n",
      "        0.22888  , -0.078148 ,  0.11172  , -0.18629  ,  0.11579  ,\n",
      "        1.0416   , -0.74788  ,  0.33463  , -0.56744  ,  0.5173   ,\n",
      "       -0.34971  ,  0.2192   , -0.067904 , -0.1231   ,  0.45712  ,\n",
      "       -0.03048  , -0.39132  , -0.11778  ,  0.14276  ,  0.91607  ,\n",
      "       -0.45087  , -0.23127  ,  0.30308  ,  0.26162  ,  0.11788  ,\n",
      "       -0.2151   , -0.25472  ,  0.51046  , -0.18225  , -0.23448  ,\n",
      "        0.63514  ,  0.48773  ,  0.13602  ,  0.077952 ,  0.048624 ,\n",
      "        0.37765  ,  0.30025  ,  0.32011  , -0.29244  , -0.060639 ,\n",
      "        0.38323  ,  0.14346  ,  0.041774 , -0.1738   ,  0.29323  ,\n",
      "       -0.18339  ,  0.89149  ,  0.7064   , -0.48768  ,  0.60736  ,\n",
      "       -0.050338 ,  0.072991 , -0.15112  , -0.45522  ,  0.33195  ,\n",
      "        1.3761   , -0.74979  ,  0.2557   ,  0.29944  , -0.045796 ,\n",
      "        0.30665  ,  0.45234  , -0.017505 , -0.51614  ,  0.47881  ,\n",
      "       -0.099928 , -0.49045  , -0.0175   , -0.0035313, -0.45266  ,\n",
      "        0.17951  , -0.53411  ,  0.36321  , -0.14557  ,  0.50801  ,\n",
      "        0.30895  ,  0.45874  ,  0.37383  , -0.22473  , -0.23064  ,\n",
      "        0.30568  , -0.15955  ,  0.23174  ,  0.22386  , -0.90902  ,\n",
      "       -0.047653 , -0.58221  ,  0.44276  ,  0.48688  , -0.020569 ,\n",
      "        0.015207 , -0.36565  , -0.056182 , -0.59864  ,  0.39156  ,\n",
      "       -0.1214   ,  0.081708 ,  0.8414   , -0.33834  , -0.57516  ,\n",
      "       -0.43614  , -0.027974 ,  0.38013  , -0.1152   , -0.15773  ,\n",
      "       -0.45423  ,  0.086106 ,  0.28252  , -0.14643  ,  0.61413  ,\n",
      "       -0.084234 , -0.13932  , -0.25247  , -0.51641  , -0.11502  ,\n",
      "        0.37162  , -0.061126 , -0.22138  , -0.11358  , -0.61499  ,\n",
      "       -0.073034 ,  0.20285  ,  0.65178  ,  0.45677  , -0.10379  ,\n",
      "        0.018831 ,  0.38626  ,  0.014366 , -0.18982  , -0.4907   ,\n",
      "       -0.044281 , -1.356    , -0.015133 ,  0.15155  , -0.073018 ,\n",
      "       -0.46388  ,  0.14422  , -0.31741  , -0.23018  , -0.414    ,\n",
      "        0.23789  , -0.085779 ,  0.21384  ,  0.3762   ,  0.34505  ,\n",
      "       -0.12657  , -0.59409  , -0.46864  , -0.32798  , -0.018532 ,\n",
      "        0.77841  , -0.19691  , -0.80945  , -0.38679  ,  0.11802  ],\n",
      "      dtype=float32)}, {'entity': 'I-per', 'word': 'know', 'embedding': array([-2.1054e-01,  1.3820e-01,  3.5328e-02,  3.9770e-02, -1.0913e-01,\n",
      "        2.4414e-01, -1.1166e-01, -2.2698e-02, -6.5657e-02, -1.7315e+00,\n",
      "        1.7997e-01, -2.6731e-01, -1.3103e-01,  2.3522e-01,  4.6647e-02,\n",
      "       -6.8261e-02, -1.8418e-01, -6.9715e-03,  4.2083e-02,  5.0421e-01,\n",
      "        4.8498e-01,  6.7052e-01,  4.9476e-01, -2.2742e-01, -4.8975e-01,\n",
      "        3.9780e-02,  4.8491e-02, -2.2711e-01, -4.5687e-02, -2.0525e-01,\n",
      "        8.4386e-02,  3.8744e-01, -6.6631e-01, -3.3196e-01, -1.1459e+00,\n",
      "        1.9380e-01, -9.7834e-02, -8.9556e-02,  6.9556e-02, -2.3474e-01,\n",
      "        3.4883e-02, -4.8816e-01, -1.4814e-02,  1.6410e-02, -2.8235e-03,\n",
      "       -3.5905e-02,  4.6645e-01, -3.8067e-02, -8.1251e-02,  3.5786e-02,\n",
      "        2.2598e-01, -2.7170e-01, -7.9388e-02,  2.7955e-02, -6.3384e-01,\n",
      "        6.6236e-01,  1.8921e-01,  2.8105e-01,  1.1652e-01,  1.1645e-01,\n",
      "        3.9587e-01, -8.4212e-02,  1.9363e-01,  7.9782e-01, -1.8834e-01,\n",
      "       -2.3503e-01,  1.5463e-01, -3.0793e-02, -3.0303e-01, -7.4742e-02,\n",
      "        1.2377e-01, -9.3509e-02, -1.8597e-01,  4.9568e-01, -5.0151e-02,\n",
      "       -1.9192e-01,  1.9253e-01,  1.6480e-01, -2.9575e-01, -1.5306e-01,\n",
      "       -1.8800e-01,  4.2239e-02,  4.8471e-01, -1.2144e-01, -1.7380e-01,\n",
      "        1.4398e-01, -2.2217e-01,  3.7578e-01, -7.2383e-02,  1.1058e-01,\n",
      "       -7.2009e-01,  4.6533e-01, -3.6602e-01, -3.9058e-01,  1.6251e-01,\n",
      "        2.0752e-01, -5.1124e-01, -2.1355e-01,  2.8441e-01, -3.8465e-01,\n",
      "        1.3870e-02, -6.5266e-04, -1.7726e-01, -1.4581e-01, -1.8602e-02,\n",
      "        1.0972e-01,  4.3777e-01,  1.8333e-02, -1.4603e-01,  5.8308e-01,\n",
      "        8.2461e-04, -4.1326e-01, -4.6429e-02, -4.2574e-01,  2.6997e-01,\n",
      "        4.7589e-01,  9.5628e-03, -1.6074e-01,  2.4485e-01, -1.2877e-02,\n",
      "       -1.0760e-01, -3.0608e-01,  2.0117e-01,  3.3389e-01,  4.7323e-02,\n",
      "       -4.0236e-01,  2.3894e-01,  4.2793e-01,  1.2448e-01, -7.2209e-02,\n",
      "        4.0496e-01, -4.5016e-01,  3.3982e-02,  7.2349e-05, -1.2840e-01,\n",
      "        1.2910e-01, -4.1094e-01, -4.6143e-02, -1.0174e-01,  2.4075e-01,\n",
      "       -7.3159e-02,  3.6907e-02,  1.7092e-01,  4.6563e-02, -4.5286e-01,\n",
      "       -2.7742e-02, -1.2161e-01, -1.1587e-01, -1.0501e-01,  3.6876e-01,\n",
      "       -6.9241e-01,  3.0956e-02,  2.1048e-01,  7.7598e-02,  6.8669e-02,\n",
      "       -1.0111e-01,  4.7341e-02, -1.5315e-01,  2.0444e-01, -1.6288e-01,\n",
      "       -2.6707e-02, -2.4677e-01,  6.9882e-02, -2.6111e-01, -5.0428e-02,\n",
      "        9.5686e-02, -5.9520e-02,  4.3476e-02, -1.4410e-01, -3.9846e-02,\n",
      "       -4.0199e-02, -4.9009e-02, -6.1524e-01, -1.1400e-01,  1.3166e-01,\n",
      "       -3.7404e-02, -2.4335e-01, -2.1840e-02,  1.9396e-01,  1.8955e-01,\n",
      "        2.2717e-01, -2.5075e-02,  1.2448e-01, -4.7739e-02, -3.0587e-01,\n",
      "        2.1145e-01,  9.4751e-02,  3.9981e-02,  6.0500e-01,  4.3323e-01,\n",
      "       -1.4758e-01,  3.1360e-03,  2.7524e-01,  9.8742e-02,  1.8057e-01,\n",
      "       -3.6113e-01,  1.2763e-02,  1.4835e-01,  1.2080e-01, -4.5139e-01,\n",
      "        1.4243e+00,  1.0923e-04,  1.9836e-01,  5.8723e-02,  2.1376e-01,\n",
      "       -1.2031e-01,  3.0895e-01,  1.5846e-01, -1.2995e-01, -4.7385e-01,\n",
      "       -4.4469e-01,  9.5954e-02,  1.5342e-01,  6.4161e-03, -6.3179e-02,\n",
      "       -1.3623e-01,  5.5599e-02,  2.1380e-01, -2.0849e-01, -1.1482e-01,\n",
      "       -1.0307e-01,  1.5589e-02,  1.1433e-01, -1.1050e-02,  4.8281e-01,\n",
      "       -5.6447e-02,  8.7733e-02,  7.2324e-02, -2.6726e-02,  9.8759e-02,\n",
      "        9.6281e-02,  1.6117e-02,  1.9775e-01, -2.2784e-01, -5.0694e-01,\n",
      "        1.7187e-01,  6.4266e-02,  1.4302e-01, -1.4108e-01, -8.6534e-02,\n",
      "        2.6254e-01, -3.9372e-02,  5.5489e-01,  8.2214e-02,  2.9996e-02,\n",
      "       -1.6053e-01, -3.8620e-01,  2.3457e-01,  3.9516e-02,  1.0434e-01,\n",
      "        2.1515e-01, -1.9747e-01, -5.4767e-01,  4.5637e-01,  4.1520e-01,\n",
      "        2.1377e-01,  3.3337e-01, -2.1027e-01, -1.8144e-01, -3.1811e-01,\n",
      "       -7.6363e-02, -1.3749e-01, -7.2385e-02, -1.0307e-01, -6.2822e-02,\n",
      "        1.9322e-01, -9.4703e-02, -1.8124e-01,  3.6122e-02,  4.0715e-01,\n",
      "        9.1011e-02, -1.8977e-01,  2.9882e-01,  2.2483e-02,  9.9356e-02,\n",
      "        1.2793e-01, -2.0860e+00,  8.8859e-02,  2.6531e-01, -7.4165e-02,\n",
      "       -5.2132e-02,  9.2726e-02,  1.0850e-01,  1.1065e-01, -1.3017e-02,\n",
      "        1.0978e-02,  2.8547e-01, -3.1178e-02, -9.3312e-02, -1.2391e-01,\n",
      "        2.5318e-01, -3.2711e-01,  1.7414e-01, -1.4468e-01,  1.0963e-01,\n",
      "       -8.2827e-01, -7.8517e-02, -1.4784e-01, -5.1009e-02,  5.2733e-01],\n",
      "      dtype=float32)}, {'entity': 'I-per', 'word': 'how', 'embedding': array([-2.8520e-01, -1.3883e-02,  3.1607e-01, -1.9182e-01,  5.9983e-02,\n",
      "        6.0524e-01, -1.8121e-01, -2.0191e-01,  5.6732e-02, -2.1441e+00,\n",
      "        2.6505e-01, -2.7387e-02, -2.6467e-01,  8.9337e-02,  2.3024e-03,\n",
      "        1.7254e-02, -2.9702e-02, -1.5041e-01, -1.8500e-02,  9.5384e-02,\n",
      "        3.8578e-01,  7.2993e-01,  2.1815e-01,  1.4281e-01, -3.5614e-01,\n",
      "       -1.1845e-01,  1.1216e-01, -7.2290e-02, -3.2908e-01,  8.8392e-02,\n",
      "        9.7741e-02,  4.5268e-01, -5.4059e-01, -3.6629e-02, -8.9579e-01,\n",
      "        3.5898e-01, -1.9695e-02,  5.7514e-02, -3.0125e-01, -4.2060e-01,\n",
      "        2.6366e-01, -3.9566e-01,  1.6363e-02,  9.2313e-02, -5.9094e-02,\n",
      "        1.0586e-01, -8.6629e-02, -1.4179e-01, -2.3508e-01,  1.4154e-01,\n",
      "        2.9091e-01, -2.6861e-01,  2.2645e-02, -3.5018e-01, -3.1623e-01,\n",
      "        2.6834e-01, -1.1712e-01,  1.6216e-01, -3.4595e-02,  2.9888e-01,\n",
      "        3.1337e-01, -4.6440e-02,  1.9017e-01,  5.1518e-01, -1.7441e-01,\n",
      "       -2.2635e-01, -2.5727e-02,  3.3247e-03, -8.2365e-02,  1.3690e-01,\n",
      "        1.5103e-01,  2.4523e-01,  1.5876e-01,  5.7382e-01,  5.0918e-02,\n",
      "       -3.4712e-01,  4.7300e-02,  9.3434e-02, -3.1719e-01, -2.8781e-01,\n",
      "       -5.6439e-01, -2.5316e-01,  4.9719e-01, -2.3592e-01, -2.7797e-01,\n",
      "        1.4790e-01,  1.2755e-02,  5.5342e-01, -9.2604e-02,  1.7752e-01,\n",
      "       -5.3235e-01,  5.6813e-01, -8.2282e-01, -3.5320e-01, -1.3537e-01,\n",
      "       -4.0542e-01, -2.8071e-01,  6.0131e-02,  1.0935e-01, -4.1172e-01,\n",
      "        6.4952e-02,  2.1935e-01, -3.5481e-02, -1.2299e-01,  4.4640e-01,\n",
      "        8.6781e-02,  5.0849e-01,  3.6674e-01, -5.4084e-02,  3.1491e-01,\n",
      "        1.7569e-01, -3.1512e-01, -1.8305e-01,  2.8661e-02,  3.9927e-02,\n",
      "        3.2165e-01,  2.0412e-01,  1.2564e-01,  3.1028e-01, -1.6373e-01,\n",
      "       -5.5296e-02, -3.0737e-01,  5.4037e-02,  8.1422e-02, -6.6234e-02,\n",
      "       -5.9576e-02, -1.5764e-01,  2.4508e-01, -1.6679e-01, -1.1627e-01,\n",
      "        3.1097e-01, -3.0654e-01, -1.1080e-01,  8.2005e-02, -2.6519e-02,\n",
      "       -1.2041e-01, -2.8812e-01, -1.2883e-01, -1.3535e-01,  1.6196e-01,\n",
      "        2.9606e-01,  1.2215e-01, -5.7976e-02,  1.1722e-01, -4.6273e-01,\n",
      "        1.2891e-01,  1.3170e-01,  2.6055e-01,  9.7484e-02,  2.3461e-01,\n",
      "       -5.9125e-01, -1.8015e-01,  4.0371e-01,  1.1619e-01,  2.1183e-01,\n",
      "       -8.7673e-02, -5.2543e-02, -5.2867e-02,  1.1351e-01, -1.8198e-01,\n",
      "        3.2583e-01, -5.0081e-01,  3.7677e-01, -1.1123e-01,  1.7776e-01,\n",
      "        3.6542e-01, -7.8592e-02,  1.0235e-01, -2.2685e-01,  1.5990e-01,\n",
      "        1.4761e-02, -1.0778e-01, -8.6391e-01, -1.4793e-01,  1.0497e-01,\n",
      "       -1.1046e-01, -2.1199e-01,  8.0026e-03,  5.6424e-02,  3.8739e-01,\n",
      "       -2.3777e-02, -1.0503e-01, -1.5014e-01, -1.8974e-01, -6.7329e-02,\n",
      "        1.8368e-01, -8.7776e-02,  4.7869e-02,  4.1803e-01,  3.0468e-01,\n",
      "       -9.5460e-02,  3.1652e-01,  8.2931e-02, -4.2179e-02,  3.2322e-02,\n",
      "       -1.2261e-01,  8.7560e-02,  2.5245e-01, -1.7277e-01, -6.8055e-01,\n",
      "        9.1765e-01,  1.2540e-02,  3.0546e-01,  1.1882e-01, -7.5455e-02,\n",
      "       -3.0937e-01,  2.0975e-01,  1.6471e-01, -1.8943e-01, -2.2963e-01,\n",
      "       -9.5508e-02,  1.4326e-03,  2.2419e-01, -6.2748e-02, -1.9195e-01,\n",
      "        1.1012e-01,  3.3455e-01,  2.8944e-01, -5.1235e-03,  1.2398e-01,\n",
      "        1.6359e-01,  2.6495e-01,  1.2624e-02,  2.9644e-01,  2.5045e-01,\n",
      "        2.7571e-01, -8.8390e-02, -7.8688e-02, -4.5731e-01,  1.1622e-02,\n",
      "        2.2839e-01,  9.4491e-02,  1.9211e-01,  4.8981e-02, -4.5525e-01,\n",
      "        1.5033e-01,  2.7869e-01,  9.8785e-03, -6.6934e-02,  1.1179e-01,\n",
      "        2.0444e-01, -1.9706e-02,  4.1337e-01,  3.5586e-01, -7.7969e-01,\n",
      "       -3.7071e-01, -3.8352e-01,  1.0505e-01,  6.7187e-02,  4.8561e-01,\n",
      "        1.8367e-01, -4.6652e-01, -2.3828e-01,  6.9951e-02,  7.6589e-01,\n",
      "        3.0080e-02, -1.4103e-01,  4.5273e-02, -9.2602e-02,  4.7313e-04,\n",
      "        6.0511e-02, -2.1033e-01, -2.2911e-01, -3.1118e-01,  2.5983e-01,\n",
      "        1.3404e-01, -3.7727e-01, -7.6947e-02, -1.8394e-02,  2.3352e-02,\n",
      "       -6.7285e-02,  6.8597e-03,  3.5925e-01,  4.5703e-01,  1.2371e-01,\n",
      "        1.0767e-01, -2.1543e+00,  1.1381e-01,  5.3105e-01, -1.8983e-01,\n",
      "       -8.3031e-02,  2.2327e-01,  2.1423e-01, -4.0043e-02,  2.3269e-02,\n",
      "       -2.2501e-01,  6.3059e-02,  1.7789e-01, -2.1471e-01, -4.4383e-02,\n",
      "        3.0500e-01, -3.0684e-01,  1.5291e-01, -2.7711e-02,  2.7281e-01,\n",
      "       -4.4360e-01, -1.5616e-01, -1.0859e-01, -1.4354e-01,  1.4850e-01],\n",
      "      dtype=float32)}, {'entity': 'I-per', 'word': \"we're\", 'embedding': array([-0.85078  ,  0.14052  ,  0.19218  , -0.56941  ,  0.012045 ,\n",
      "       -0.13574  ,  0.026476 ,  0.63774  ,  0.13145  , -1.5406   ,\n",
      "        0.022069 , -0.093937 , -0.0075906,  0.18398  ,  0.098117 ,\n",
      "        0.34659  , -0.29379  , -0.023845 ,  0.34028  , -0.16479  ,\n",
      "       -0.4635   , -0.7426   , -0.098289 , -0.66905  , -0.20917  ,\n",
      "       -0.097382 , -0.028347 ,  0.5289   , -0.1507   ,  0.52132  ,\n",
      "        0.096284 , -0.20201  , -0.23403  ,  0.35867  ,  0.14002  ,\n",
      "        0.13554  ,  0.15319  ,  0.32805  ,  0.020936 ,  0.1657   ,\n",
      "       -0.0089688,  0.059181 , -0.1375   ,  0.035753 , -0.17763  ,\n",
      "        0.12395  ,  0.041585 ,  0.51095  ,  0.11174  ,  0.030507 ,\n",
      "       -0.079747 , -0.70903  , -0.60473  ,  0.36105  , -0.16732  ,\n",
      "       -0.044104 ,  0.10538  ,  0.39964  ,  0.22989  ,  0.31049  ,\n",
      "       -0.24369  , -0.33433  ,  0.3908   , -0.52597  , -0.44613  ,\n",
      "       -0.055256 , -0.084394 , -0.28462  ,  0.36583  ,  0.62191  ,\n",
      "       -0.36241  ,  0.054556 ,  0.42029  ,  0.2916   , -0.68549  ,\n",
      "        0.29185  ,  0.60148  , -0.30741  ,  0.4329   , -0.27715  ,\n",
      "        0.40411  ,  0.27686  , -0.13243  , -0.31177  ,  0.061525 ,\n",
      "       -0.15922  ,  0.19996  , -0.39375  ,  0.25412  , -0.10159  ,\n",
      "        0.6096   ,  0.12271  , -0.59604  , -0.31485  , -0.32325  ,\n",
      "       -0.14606  , -0.25834  ,  0.20505  ,  0.1371   ,  0.19961  ,\n",
      "       -0.1093   , -0.24252  ,  0.10222  ,  0.21871  ,  0.037622 ,\n",
      "       -0.80619  ,  0.56828  ,  0.18972  , -0.18194  ,  0.57378  ,\n",
      "        0.27703  , -0.99991  , -0.15494  , -0.80141  , -0.031519 ,\n",
      "        0.077413 , -0.38559  ,  0.33344  , -0.34985  , -0.0061697,\n",
      "        0.26873  , -0.012963 ,  0.43523  , -0.35281  , -0.18652  ,\n",
      "       -0.12175  ,  0.024119 ,  0.075614 ,  0.27854  , -0.30625  ,\n",
      "       -0.3751   ,  0.48567  , -0.90504  , -0.086511 ,  0.2357   ,\n",
      "       -0.51093  , -0.28514  , -0.25993  ,  0.70909  , -0.41138  ,\n",
      "       -0.41395  , -0.0059017, -0.31946  ,  0.0089687, -0.34839  ,\n",
      "        0.22888  , -0.078148 ,  0.11172  , -0.18629  ,  0.11579  ,\n",
      "        1.0416   , -0.74788  ,  0.33463  , -0.56744  ,  0.5173   ,\n",
      "       -0.34971  ,  0.2192   , -0.067904 , -0.1231   ,  0.45712  ,\n",
      "       -0.03048  , -0.39132  , -0.11778  ,  0.14276  ,  0.91607  ,\n",
      "       -0.45087  , -0.23127  ,  0.30308  ,  0.26162  ,  0.11788  ,\n",
      "       -0.2151   , -0.25472  ,  0.51046  , -0.18225  , -0.23448  ,\n",
      "        0.63514  ,  0.48773  ,  0.13602  ,  0.077952 ,  0.048624 ,\n",
      "        0.37765  ,  0.30025  ,  0.32011  , -0.29244  , -0.060639 ,\n",
      "        0.38323  ,  0.14346  ,  0.041774 , -0.1738   ,  0.29323  ,\n",
      "       -0.18339  ,  0.89149  ,  0.7064   , -0.48768  ,  0.60736  ,\n",
      "       -0.050338 ,  0.072991 , -0.15112  , -0.45522  ,  0.33195  ,\n",
      "        1.3761   , -0.74979  ,  0.2557   ,  0.29944  , -0.045796 ,\n",
      "        0.30665  ,  0.45234  , -0.017505 , -0.51614  ,  0.47881  ,\n",
      "       -0.099928 , -0.49045  , -0.0175   , -0.0035313, -0.45266  ,\n",
      "        0.17951  , -0.53411  ,  0.36321  , -0.14557  ,  0.50801  ,\n",
      "        0.30895  ,  0.45874  ,  0.37383  , -0.22473  , -0.23064  ,\n",
      "        0.30568  , -0.15955  ,  0.23174  ,  0.22386  , -0.90902  ,\n",
      "       -0.047653 , -0.58221  ,  0.44276  ,  0.48688  , -0.020569 ,\n",
      "        0.015207 , -0.36565  , -0.056182 , -0.59864  ,  0.39156  ,\n",
      "       -0.1214   ,  0.081708 ,  0.8414   , -0.33834  , -0.57516  ,\n",
      "       -0.43614  , -0.027974 ,  0.38013  , -0.1152   , -0.15773  ,\n",
      "       -0.45423  ,  0.086106 ,  0.28252  , -0.14643  ,  0.61413  ,\n",
      "       -0.084234 , -0.13932  , -0.25247  , -0.51641  , -0.11502  ,\n",
      "        0.37162  , -0.061126 , -0.22138  , -0.11358  , -0.61499  ,\n",
      "       -0.073034 ,  0.20285  ,  0.65178  ,  0.45677  , -0.10379  ,\n",
      "        0.018831 ,  0.38626  ,  0.014366 , -0.18982  , -0.4907   ,\n",
      "       -0.044281 , -1.356    , -0.015133 ,  0.15155  , -0.073018 ,\n",
      "       -0.46388  ,  0.14422  , -0.31741  , -0.23018  , -0.414    ,\n",
      "        0.23789  , -0.085779 ,  0.21384  ,  0.3762   ,  0.34505  ,\n",
      "       -0.12657  , -0.59409  , -0.46864  , -0.32798  , -0.018532 ,\n",
      "        0.77841  , -0.19691  , -0.80945  , -0.38679  ,  0.11802  ],\n",
      "      dtype=float32)}] {'English.': (1.0, 'Tibetan'), \"don't\": (1.0, 'DalaiLama')}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "best_anec = None\n",
    "best_cos = -10\n",
    "best_text_ner = None\n",
    "best_simmilarity = None\n",
    "best_filtered = None\n",
    "swear_flag = True\n",
    "\n",
    "if swear_flag:\n",
    "    resulted_anec_list = english_anecs_prepared + translated_anecs_prepared\n",
    "else:\n",
    "    resulted_anec_list = english_anecs_prepared\n",
    "\n",
    "filtred_text = get_non_o(get_ners(text))\n",
    "text_embeddings = get_embeddings(filtred_text)\n",
    "for anec_embed, anec in tqdm(resulted_anec_list):\n",
    "    try:\n",
    "        new_cosine, new_simmilarity = count_cos_embeddings(text_embeddings, anec_embed)\n",
    "        if new_cosine > best_cos:\n",
    "            best_cos = new_cosine\n",
    "            best_filtered = anec_embed\n",
    "            best_text_ner = filtred_text\n",
    "            best_anec = anec\n",
    "            best_simmilarity = new_simmilarity\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "print(best_anec,best_cos, best_text_ner, best_filtered, best_simmilarity)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['English.', \"don't\"])\n",
      "Some of the most tactful people on Earth are Tibetan One office supervisor called a secretary in to give her the bad news that she was being fired. He started the conversation with: \"Miss Symthe, I really DalaiLama know how we're going to get along without you, but starting Monday, we're going to try.\n"
     ]
    }
   ],
   "source": [
    "words = best_anec.split()\n",
    "key_words = best_simmilarity.keys()\n",
    "print(key_words)\n",
    "for i in range(len(words)):\n",
    "    if words[i] in key_words:\n",
    "        words[i] = best_simmilarity[words[i]][1]\n",
    "resulted_text = \" \".join(words)\n",
    "print(resulted_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
