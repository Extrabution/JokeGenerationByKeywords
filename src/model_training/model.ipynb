{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "\n",
    "with open(\"../../data/article_texts.txt\",'rb') as f:\n",
    "    texts = pickle.load(f, encoding=\"UTF-8\")\n",
    "with open(\"../../data/english_anecs_list.pickle\", \"rb\") as f:\n",
    "    english_anecs_list = pickle.load(f, encoding=\"UTF-8\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "text = ['SpaceX Starship Blows Up Minutes After Launch',\n",
    " 'SpaceX’s Starship rocket, the most powerful ever built, blasted off on an unpiloted maiden flight Thursday, flying for more than two minutes before exploding. What do you think?']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3607c3f6f4f54ee2920f5f1b45b0145e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize(data:str):\n",
    "    inputs = tokenizer(data, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    return inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': tensor([[  101,  2686,  2595,  3340,  5605, 13783,  2039,  2781,  2044,  4888,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, {'input_ids': tensor([[  101,  2686,  2595,  1521,  1055,  3340,  5605,  7596,  1010,  1996,\n",
      "          2087,  3928,  2412,  2328,  1010, 18461,  2125,  2006,  2019,  4895,\n",
      "          8197, 10994,  2098, 10494,  3462,  9432,  1010,  3909,  2005,  2062,\n",
      "          2084,  2048,  2781,  2077, 20728,  1012,  2054,  2079,  2017,  2228,\n",
      "          1029,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}]\n",
      "[CLS] spacex starship blows up minutes after launch [SEP]\n",
      "['[CLS]', 'space', '##x', 'stars', '##hip', 'blows', 'up', 'minutes', 'after', 'launch', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = []\n",
    "for i in text:\n",
    "    tokenized_text.append(tokenize(i))\n",
    "print(tokenized_text)\n",
    "print(tokenizer.decode(tokenized_text[0].input_ids[0]))\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_text[0].input_ids[0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TokenClassificationPipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "pos_model = AutoModelForTokenClassification.\\\n",
    "    from_pretrained(\"QCRI/bert-base-multilingual-cased-pos-english\")\n",
    "pos_pipeline = TokenClassificationPipeline(model=pos_model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "ner_pipeline = pipeline(\"ner\", model=ner_model, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.9989837, 'index': 1, 'word': 'netflix', 'start': 0, 'end': 7}, {'entity': 'B-PER', 'score': 0.9984754, 'index': 21, 'word': 'trail', 'start': 94, 'end': 99}, {'entity': 'I-PER', 'score': 0.99533194, 'index': 22, 'word': '##bla', 'start': 99, 'end': 102}] Netflix announced it will be ending its DVD-by-mail rental service that set the stage for its trailblazing video streaming service, ending an era that began 25 years ago when delivering discs through the mail was considered a revolutionary concept. What do you think? ['netflix', 'trail', '##bla']\n"
     ]
    }
   ],
   "source": [
    "example = texts[1][1]\n",
    "ner_results = ner_pipeline(example)\n",
    "\n",
    "keywords = [result[\"word\"] for result in ner_results]\n",
    "print(ner_results, example, keywords)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "['NNP', 'FW', 'FW']"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags = pos_pipeline(texts[1][1])\n",
    "unique_pos_tags = []\n",
    "for pos_tag in pos_tags:\n",
    "    if pos_tag[\"word\"] in keywords:\n",
    "        unique_pos_tags.append(pos_tag[\"entity\"])\n",
    "unique_pos_tags"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "['JJ',\n 'JJ',\n 'FW',\n 'FW',\n 'NN',\n 'FW',\n 'FW',\n 'FW',\n 'FW',\n 'NN',\n 'FW',\n 'NN',\n 'FW',\n 'CD',\n 'NN',\n 'FW',\n 'FW',\n 'FW',\n 'CD',\n 'CD',\n 'CD',\n 'FW',\n ':',\n 'FW',\n 'FW',\n 'FW',\n 'CD',\n 'FW',\n 'JJ',\n 'FW',\n 'FW',\n 'FW',\n 'NN',\n 'FW',\n 'NN',\n 'FW',\n 'FW',\n 'FW',\n 'NN',\n 'FW',\n 'FW',\n 'FW',\n 'NN',\n 'FW',\n 'FW',\n 'FW',\n 'FW',\n 'FW',\n 'FW',\n 'NN',\n 'FW',\n 'FW',\n 'FW',\n 'CD',\n 'NN',\n ':',\n 'FW',\n 'FW',\n 'FW',\n 'FW',\n 'CD',\n 'FW',\n 'JJ',\n 'NN',\n 'NN',\n 'CD',\n 'NN',\n 'NN',\n 'CD',\n 'FW',\n 'FW',\n 'NN',\n 'NN',\n 'CD',\n 'NN',\n 'FW',\n 'FW',\n 'NN',\n 'FW',\n ':',\n 'FW',\n 'FW',\n 'FW',\n 'NN',\n 'NN',\n 'NN',\n 'CD',\n 'FW',\n 'FW',\n 'NN',\n 'NN',\n 'CD',\n 'NN',\n 'NNS',\n ':',\n 'FW',\n 'NN',\n 'FW',\n 'FW',\n 'NN',\n 'NN',\n 'CD',\n 'FW',\n 'FW',\n 'FW',\n 'NN',\n 'FW']"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_template_result = pos_pipeline(english_anecs_list[0])\n",
    "pos_template_tags = []\n",
    "for pos_tag in pos_template_result:\n",
    "    pos_template_tags.append(pos_tag[\"entity\"])\n",
    "pos_template_tags"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
